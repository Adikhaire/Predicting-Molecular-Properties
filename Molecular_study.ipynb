{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:58:34.924736Z",
     "start_time": "2019-08-06T15:58:34.112516Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn import neural_network\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:58:42.708800Z",
     "start_time": "2019-08-06T15:58:35.742684Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add train set \n",
    "train = pd.read_csv('../champs-scalar-coupling/train.csv')\n",
    "test = pd.read_csv('../champs-scalar-coupling/test.csv')\n",
    "structures = pd.read_csv('../champs-scalar-coupling/structures.csv')\n",
    "submission = pd.read_csv('../champs-scalar-coupling/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:58:58.197418Z",
     "start_time": "2019-08-06T15:58:47.374454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging all the data together\n",
    "def merging_structure_train_test(data, structures, atom_idx):\n",
    "    df = pd.merge(data, structures, how='left', \n",
    "                  left_on=['molecule_name', f'atom_index_{atom_idx}'], right_on=['molecule_name', 'atom_index'])\n",
    "    df = df.drop(labels=['atom_index'], axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "merged_train_data = merging_structure_train_test(train, structures=structures, atom_idx=0)\n",
    "merged_train_data = merging_structure_train_test(merged_train_data, structures=structures, atom_idx=1)\n",
    "merged_train_data.drop(labels=['atom_0', 'atom_1'], axis=1, inplace=True)\n",
    "\n",
    "merged_test_data = merging_structure_train_test(test, structures=structures, atom_idx=0)\n",
    "merged_test_data = merging_structure_train_test(merged_test_data, structures=structures, atom_idx=1)\n",
    "merged_test_data.drop(labels=['atom_0', 'atom_1'], axis=1, inplace=True)\n",
    "\n",
    "# Calculate the distance between xyz and \n",
    "merged_train_data['distance'] = (merged_train_data['x_0'] - merged_train_data['x_1'])**2 + (merged_train_data['y_0'] - merged_train_data['y_1'])**2 + (merged_train_data['z_0'] - merged_train_data['z_1'])**2        \n",
    "merged_train_data['distance'] = np.sqrt(merged_train_data['distance'])\n",
    "\n",
    "merged_test_data['distance'] = (merged_test_data['x_0'] - merged_test_data['x_1'])**2 + (merged_test_data['y_0'] - merged_test_data['y_1'])**2 + (merged_test_data['z_0'] - merged_test_data['z_1'])**2        \n",
    "merged_test_data['distance'] = np.sqrt(merged_test_data['distance'])\n",
    "\n",
    "merged_train_data.drop(labels=['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1, inplace=True)\n",
    "merged_test_data.drop(labels=['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1, inplace=True)\n",
    "\n",
    "# Convert the textual data into categorical data. \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "merged_train_data['molecule_name'] = le.fit_transform(merged_train_data['molecule_name'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "merged_train_data['type'] = le.fit_transform(merged_train_data['type'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "merged_test_data['molecule_name'] = le.fit_transform(merged_test_data['molecule_name'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "merged_test_data['type'] = le.fit_transform(merged_test_data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:59:27.978540Z",
     "start_time": "2019-08-06T15:59:27.129004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the training and testing dataset to generate the X and y columns. \n",
    "train_x = np.array(merged_train_data.drop(labels=['id', 'scalar_coupling_constant'], axis=1))\n",
    "train_y = np.around(np.array(merged_train_data['scalar_coupling_constant']).reshape((train_x.shape[0], )), decimals=4)\n",
    "print(\"Shape of the train_x\", train_x.shape)\n",
    "print(\"Shape of the train_y\", train_y.shape )\n",
    "\n",
    "# Testing data\n",
    "test_x = np.array(merged_test_data.iloc[:, 1:])\n",
    "print(\"Shape of the test_x\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T11:40:03.891941Z",
     "start_time": "2019-08-07T11:31:16.355262Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Neural Network using sklearn\n",
    "print(\"Starting the Neural Network model\")\n",
    "# Importing all the required libraies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn import neural_network\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "neurons = train_x.shape[0]/(9*(train_x.shape[1] + 1))\n",
    "# neurons\n",
    "nnl = neural_network.MLPRegressor()\n",
    "\n",
    "# Provide parameters to the neural network\n",
    "hidden_layer_sizes = [(10, )]\n",
    "max_iteration = [10, 15]\n",
    "learning_rate = [\"adaptive\"]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iteration, learning_rate=learning_rate)\n",
    "# Running Grid search on the neural network model with mean squared error and cross fold and n_jobs\n",
    "grid_search = GridSearchCV(nnl, param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=1)\n",
    "grid_result = grid_search.fit(train_x, train_y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "plt.errorbar(max_iteration, means, yerr=stds)\n",
    "plt.title(\"Neural Network losses\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('max_depth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T14:26:35.467861Z",
     "start_time": "2019-08-04T14:21:33.415555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forrest Regression\n",
    "\n",
    "# Importing all the required libraies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import ensemble\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# neurons\n",
    "rfr = ensemble.RandomForestRegressor()\n",
    "\n",
    "# Provide parameters to the neural network\n",
    "n_estimaters = 100\n",
    "max_depth = train_x.shape[1]\n",
    "param_grid = dict(n_estimaters=n_estimaters, max_depth=max_depth)\n",
    "# Running Grid search on the neural network model with mean squared error and cross fold and n_jobs\n",
    "grid_search = GridSearchCV(rfr, param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=1)\n",
    "grid_result = grid_search.fit(train_x, train_y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"Neural Network losses\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('max_depth.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:53:05.831466Z",
     "start_time": "2019-08-07T07:43:23.799343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implementing Xgboost model\n",
    "# Importing all the required libraies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import ensemble\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Convert the train data into dmatrix\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "\n",
    "bst = xgb.XGBRegressor(n_estimators=100)\n",
    "max_depth = range(1, train_x.shape[1]+1, 1)\n",
    "print(max_depth)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "grid_search = GridSearchCV(bst, param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=2, verbose=1)\n",
    "grid_result = grid_search.fit(train_x, train_y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"XGBoost max_depth vs Log Loss\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('max_depth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T08:04:24.986746Z",
     "start_time": "2019-08-07T08:04:09.370864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Submission Report\n",
    "submission.drop(labels=['scalar_coupling_constant'], axis=1, inplace=True)\n",
    "submission['scalar_coupling_constant'] = grid_search.predict(test_x)\n",
    "submission.to_csv('final_submission_'+ str(datetime.datetime.now()) + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
