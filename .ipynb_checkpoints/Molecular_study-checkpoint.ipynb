{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T09:27:53.480544Z",
     "start_time": "2019-08-10T09:27:51.425964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn import neural_network\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T09:29:22.485438Z",
     "start_time": "2019-08-10T09:29:15.559859Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Add train set\n",
    "def load_dataset(path):\n",
    "    train = pd.read_csv(path + 'train.csv')\n",
    "    test = pd.read_csv(path + 'test.csv')\n",
    "    structures = pd.read_csv(path + 'structures.csv')\n",
    "    submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "    return train, test, structures, submission\n",
    "\n",
    "train, test, structures, submission = load_dataset(\n",
    "    path=\n",
    "    '/Users/ak/Documents/personal/Personal/molecular/champs-scalar-coupling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T05:21:43.834218Z",
     "start_time": "2019-08-08T05:21:27.983213Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging all the data together\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def merging_structure_train_test(data, structures, atom_idx):\n",
    "    df = pd.merge(data,\n",
    "                  structures,\n",
    "                  how='left',\n",
    "                  left_on=['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on=['molecule_name', 'atom_index'])\n",
    "    df = df.drop(labels=['atom_index'], axis=1)\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            'atom': f'atom_{atom_idx}',\n",
    "            'x': f'x_{atom_idx}',\n",
    "            'y': f'y_{atom_idx}',\n",
    "            'z': f'z_{atom_idx}'\n",
    "        })\n",
    "    return df\n",
    "\n",
    "\n",
    "def merging_data(train, test):\n",
    "\n",
    "    merged_train_data = merging_structure_train_test(train,\n",
    "                                                     structures=structures,\n",
    "                                                     atom_idx=0)\n",
    "    merged_train_data = merging_structure_train_test(merged_train_data,\n",
    "                                                     structures=structures,\n",
    "                                                     atom_idx=1)\n",
    "    merged_train_data.drop(labels=['atom_0', 'atom_1'], axis=1, inplace=True)\n",
    "\n",
    "    merged_test_data = merging_structure_train_test(test,\n",
    "                                                    structures=structures,\n",
    "                                                    atom_idx=0)\n",
    "    merged_test_data = merging_structure_train_test(merged_test_data,\n",
    "                                                    structures=structures,\n",
    "                                                    atom_idx=1)\n",
    "    merged_test_data.drop(labels=['atom_0', 'atom_1'], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate the distance between xyz and\n",
    "    merged_train_data['distance'] = (\n",
    "        merged_train_data['x_0'] - merged_train_data['x_1'])**2 + (\n",
    "            merged_train_data['y_0'] - merged_train_data['y_1'])**2 + (\n",
    "                merged_train_data['z_0'] - merged_train_data['z_1'])**2\n",
    "    merged_train_data['distance'] = np.sqrt(merged_train_data['distance'])\n",
    "\n",
    "    merged_test_data['distance'] = (\n",
    "        merged_test_data['x_0'] - merged_test_data['x_1'])**2 + (\n",
    "            merged_test_data['y_0'] - merged_test_data['y_1'])**2 + (\n",
    "                merged_test_data['z_0'] - merged_test_data['z_1'])**2\n",
    "    merged_test_data['distance'] = np.sqrt(merged_test_data['distance'])\n",
    "\n",
    "    merged_train_data.drop(labels=['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'],\n",
    "                           axis=1,\n",
    "                           inplace=True)\n",
    "    merged_test_data.drop(labels=['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'],\n",
    "                          axis=1,\n",
    "                          inplace=True)\n",
    "    \n",
    "    return merged_train_data, merged_test_data\n",
    "\n",
    "# Convert the textual data into categorical data\n",
    "def preporocessing_data(merged_train_data, merged_test_data):\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    merged_train_data['molecule_name'] = le.fit_transform(\n",
    "        merged_train_data['molecule_name'])\n",
    "    merged_train_data['type'] = le.fit_transform(merged_train_data['type'])\n",
    "    merged_test_data['molecule_name'] = le.fit_transform(\n",
    "        merged_test_data['molecule_name'])\n",
    "    merged_test_data['type'] = le.fit_transform(merged_test_data['type'])\n",
    "    return merged_train_data, merged_test_data\n",
    "\n",
    "merged_train, merged_test = merging_data(train, test)\n",
    "merged_train, merged_test = preporocessing_data(merged_train, merged_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T05:48:27.268978Z",
     "start_time": "2019-08-08T05:48:26.554528Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Splitting the training and testing dataset to generate the X and y columns.\n",
    "def split_data(merged_train_data, merged_test_data):\n",
    "    train_x = np.array(\n",
    "        merged_train_data.drop(\n",
    "            labels=['id', 'molecule_name', 'scalar_coupling_constant'], axis=1))\n",
    "    train_y = np.around(np.array(\n",
    "        merged_train_data['scalar_coupling_constant']).reshape(\n",
    "            (train_x.shape[0], )),\n",
    "                        decimals=4)\n",
    "\n",
    "    # Perform Normalization on the training dataset\n",
    "    from sklearn.preprocessing import normalize\n",
    "    train_x = normalize(train_x)\n",
    "\n",
    "    print(\"Shape of the train_x\", train_x.shape)\n",
    "    print(\"Shape of the train_y\", train_y.shape)\n",
    "\n",
    "    # Testing data\n",
    "    test_x = np.array(merged_test_data.iloc[:, 2:])\n",
    "    print(\"Shape of the test_x\", test_x.shape)\n",
    "    return train_x, train_y, test_x\n",
    "\n",
    "train_x, train_y, test_x = split_data(merged_train, merged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T06:15:17.794337Z",
     "start_time": "2019-08-08T06:02:26.926153Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Neural Network using sklearn\n",
    "def nnl(train_x, train_y):\n",
    "    print(\"Starting the Neural Network model\")\n",
    "    # Importing all the required libraies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import sklearn\n",
    "    from sklearn import neural_network\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # Selection of neurons\n",
    "    neurons = train_x.shape[0] / (9 * (train_x.shape[1] + 1))\n",
    "    \n",
    "    # neurons\n",
    "    nnl = neural_network.MLPRegressor()\n",
    "\n",
    "    # Provide parameters to the neural network\n",
    "    hidden_layer_sizes = [(10, 10, 5)]\n",
    "    max_iteration = [10, 15]\n",
    "    learning_rate = [\"adaptive\"]\n",
    "    param_grid = dict(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      max_iter=max_iteration,\n",
    "                      learning_rate=learning_rate)\n",
    "    # Running Grid search on the neural network model with mean squared error and cross fold and n_jobs\n",
    "    grid_search = GridSearchCV(nnl,\n",
    "                               param_grid,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=1)\n",
    "    grid_result = grid_search.fit(train_x, train_y)\n",
    "    print(\"Best: %f using %s\" %\n",
    "          (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    # plot\n",
    "    plt.errorbar(max_iteration, means, yerr=stds)\n",
    "    plt.title(\"Neural Network losses\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.savefig('max_depth.png')\n",
    "    return grid_search\n",
    "\n",
    "grid_search_nnl = nnl(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T07:39:38.308675Z",
     "start_time": "2019-08-08T07:15:22.816921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forrest Regression\n",
    "\n",
    "# Importing all the required libraies\n",
    "def rfr(train_x, train_y):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn import ensemble\n",
    "    import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "    import plotly.tools as tls\n",
    "    from plotly.offline import iplot, init_notebook_mode\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    from functools import reduce\n",
    "    pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "    pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "    pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # neurons\n",
    "    rfr = ensemble.RandomForestRegressor()\n",
    "\n",
    "    # Provide parameters to the neural network\n",
    "    n_estimaters = [100, 150]\n",
    "    max_depth = [train_x.shape[1]]\n",
    "    param_grid = dict(n_estimators=n_estimaters)\n",
    "    # Running Grid search on the neural network model with mean squared error and cross fold and n_jobs\n",
    "    grid_search = GridSearchCV(rfr,\n",
    "                               param_grid,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               cv=5,\n",
    "                               verbose=1)\n",
    "    grid_result = grid_search.fit(train_x, train_y)\n",
    "    print(\"Best: %f using %s\" %\n",
    "          (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    # plot\n",
    "    plt.errorbar(n_estimaters, means, yerr=stds)\n",
    "    plt.title(\"Neural Network losses\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.savefig('max_depth.png')\n",
    "    return grid_search\n",
    "\n",
    "grid_search_rfr = rfr(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T05:56:05.820341Z",
     "start_time": "2019-08-08T05:48:38.132188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implementing Xgboost model\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn import ensemble\n",
    "    import datetime\n",
    "    import Xgboost as xgb\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "# Importing all the required libraies\n",
    "def xgb(train_x, train_y):\n",
    "    bst = xgb.XGBRegressor(n_estimators=100)\n",
    "    max_depth = range(1, train_x.shape[1] + 1, 1)\n",
    "    print(max_depth)\n",
    "    param_grid = dict(max_depth=max_depth)\n",
    "    grid_search = GridSearchCV(bst,\n",
    "                               param_grid,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               cv=2,\n",
    "                               verbose=1)\n",
    "    grid_result = grid_search.fit(train_x, train_y)\n",
    "    print(\"Best: %f using %s\" %\n",
    "          (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    # plot\n",
    "    plt.errorbar(max_depth, means, yerr=stds)\n",
    "    plt.title(\"XGBoost max_depth vs Log Loss\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.savefig('max_depth.png')\n",
    "    return grid_search\n",
    "\n",
    "grid_search_xgb = xgb(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T08:04:24.986746Z",
     "start_time": "2019-08-07T08:04:09.370864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submission Report\n",
    "def submission_report(grid_search, test_x):\n",
    "    \n",
    "    submission.drop(labels=['scalar_coupling_constant'], axis=1, inplace=True)\n",
    "    submission['scalar_coupling_constant'] = grid_search.predict(test_x)\n",
    "    submission.to_csv('final_submission_' + str(datetime.datetime.now()) + '.csv',\n",
    "                      index=False)\n",
    "    return True\n",
    "\n",
    "submission_report(grid_search, test_x)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
